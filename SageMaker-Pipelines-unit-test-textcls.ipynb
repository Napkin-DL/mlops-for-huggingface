{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Pipelines의 Unit Testing 하기 - TextCls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# pipeline_session = PipelineSession(\n",
    "#     boto_session=boto_session,\n",
    "#     sagemaker_client=sagemaker_client,\n",
    "#     default_bucket=default_bucket,\n",
    "# )\n",
    "\n",
    "pipeline_session = LocalPipelineSession(\n",
    "    boto_session=boto_session,\n",
    "    default_bucket=default_bucket,\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"PT12H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.join(os.getcwd(), 'pipelines/textcls/')\n",
    "BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.retry import (\n",
    "    StepRetryPolicy,\n",
    "    StepExceptionTypeEnum,\n",
    "    SageMakerJobStepRetryPolicy,\n",
    "    SageMakerJobExceptionTypeEnum\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요 Packages import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Parameters 정의에 필요한 Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Processing에 필요한 Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    "    ScriptProcessor,\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.huggingface import HuggingFaceProcessor\n",
    "from sagemaker.pytorch import PyTorchProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Training에 필요한 Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.huggingface import HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. Evaluation에 필요한 Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-5. Model Metrics에 필요한 Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import (\n",
    "    MetricsSource,\n",
    "    ModelMetrics,\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.functions import (\n",
    "    JsonGet,\n",
    "    Join,\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.workflow.step_collections import RegisterModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. get_pipeline의 입력 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package_group_name=\"TextClsPackageGroup\"\n",
    "pipeline_name=\"TextclsPipeline\"\n",
    "base_job_prefix=\"Textcls\"\n",
    "processing_instance_type=\"ml.m5.xlarge\"\n",
    "training_instance_type=\"ml.c5.9xlarge\"\n",
    "\n",
    "# s3_input_prefix = 'a2i-output'\n",
    "s3_output_prefix = 'hf_processing_output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. 모델 빌딩 파이프라인 스텝(Step) 정의\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. 모델 빌딩 파이프라인 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here we define which exceptions to capture and when to retry the step\n",
    "step_retry_policy = StepRetryPolicy(\n",
    "    exception_types=[\n",
    "        StepExceptionTypeEnum.SERVICE_FAULT,\n",
    "        StepExceptionTypeEnum.THROTTLING,\n",
    "    ],\n",
    "    backoff_rate=2.0, # the multiplier by which the retry interval increases during each attempt\n",
    "    interval_seconds=30, # the number of seconds before the first retry attempt\n",
    "    expire_after_mins=4*60  # keep trying for for 4 hours max\n",
    ")\n",
    "\n",
    "job_retry_policy = SageMakerJobStepRetryPolicy(\n",
    "    exception_types=[SageMakerJobExceptionTypeEnum.RESOURCE_LIMIT],\n",
    "    failure_reason_types=[\n",
    "        SageMakerJobExceptionTypeEnum.INTERNAL_ERROR,\n",
    "        SageMakerJobExceptionTypeEnum.CAPACITY_ERROR,\n",
    "    ],\n",
    "    backoff_rate=2.0, # the multiplier by which the retry interval increases during each attempt\n",
    "    interval_seconds=30, # the number of seconds before the first retry attempt\n",
    "    expire_after_mins=4*60  # keep trying for for 4 hours max\n",
    ")\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"PT1H\")\n",
    "\n",
    "# parameters for pipeline execution\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "training_instance_count = ParameterInteger(name=\"TrainingInstanceCount\", default_value=1)\n",
    "processing_instance_type = \"ml.m5.xlarge\"\n",
    "training_instance_type = \"ml.c5.9xlarge\"\n",
    "model_approval_status = ParameterString(name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. 전처리 스텝 단계 정의\n",
    "\n",
    "크게 아래와 같은 순서로 정의 합니다.\n",
    "- 프로세싱 오브젝트 정의 (SKLearnProcessor)\n",
    "- 프로세싱 스텝 정의\n",
    "    - 일력 데이터 세트\n",
    "        - source: S3 경로 (input_data_uri)\n",
    "        - destination: 도커 컨테이너의 내부 폴더 위치\n",
    "    - 출력 위치\n",
    "        - 훈련 전처리 데이터 결과 위치\n",
    "        - 테스트 전처리 데이터 결과 위치\n",
    "    - 프로세싱 코드\n",
    "    - 프로세싱 코드에 넘길 인자 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# processing step for feature engineering\n",
    "pre_processor = PyTorchProcessor(\n",
    "    role=role, \n",
    "    instance_count=processing_instance_count,\n",
    "    instance_type=processing_instance_type,\n",
    "    framework_version='1.8',\n",
    "    base_job_name='PreprocessingforHF',\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "processor_args = pre_processor.run(\n",
    "                        code='processing-script.py',\n",
    "                        source_dir='scripts',\n",
    "                        outputs=[\n",
    "                            ProcessingOutput(\n",
    "                                output_name='train', \n",
    "                                source='/opt/ml/processing/output/train/',\n",
    "                                destination=f's3://{default_bucket}/{s3_output_prefix}/train'),\n",
    "                            ProcessingOutput(\n",
    "                                output_name='test', \n",
    "                                source='/opt/ml/processing/output/test/', \n",
    "                                destination=f's3://{default_bucket}/{s3_output_prefix}/test'),\n",
    "                        ]\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"PrepareAugmentedData\", \n",
    "    step_args=processor_args,\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련의 입력으로 사용할 이전 단계의 Processing 결과는 아래 형태로 제공됩니다.\n",
    "- `step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sm_pipelines_exec as sm_exec\n",
    "\n",
    "test_parameters_list = [processing_instance_count, processing_instance_type, training_instance_type, training_instance_count, model_approval_status]\n",
    "test_steps_list_process = [step_process]\n",
    "\n",
    "execution_preprocess = sm_exec.exec_pipelines(pipeline_name, role, test_parameters_list, test_steps_list_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_exec.describe_pipelines(execution_preprocess)\n",
    "sm_exec.get_step_results(execution_preprocess,test_steps_list_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 학습을 위한 학습단계 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'train_batch_size': 32,\n",
    "                 'model_name':'distilbert-base-uncased'\n",
    "                 }\n",
    "\n",
    "# image_uri=\"763104351884.dkr.ecr.eu-west-1.amazonaws.com/huggingface-pytorch-training:1.7-transformers4.6-gpu-py36-cu110-ubuntu18.04\"\n",
    "train_image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-training:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\"\n",
    "\n",
    "# training step for generating model artifacts\n",
    "model_path = f\"s3://{default_bucket}/{s3_output_prefix}/train_result\"\n",
    "\n",
    "\n",
    "#     metric_definitions = [\n",
    "#         {'Name': 'TrainLoss', 'Regex': r'\\'loss\\':([-+]?[0-9]*\\.?[0-9]+(?:[eE][-+]?[0-9]+)?),'},\n",
    "#         {'Name': 'EvalLoss', 'Regex': r'\\'eval_loss\\':([-+]?[0-9]*\\.?[0-9]+(?:[eE][-+]?[0-9]+)?),'},\n",
    "#         {'Name': 'EvalAcc', 'Regex': r'\\'eval_accuracy\\':([-+]?[0-9]*\\.?[0-9]+(?:[eE][-+]?[0-9]+)?),'},\n",
    "#         {'Name': 'EvalF1', 'Regex': r'\\'eval_f1\\':([-+]?[0-9]*\\.?[0-9]+(?:[eE][-+]?[0-9]+)?),'},\n",
    "#         {'Name': 'EvalPrecision', 'Regex': r'\\'eval_precision\\':([-+]?[0-9]*\\.?[0-9]+(?:[eE][-+]?[0-9]+)?),'},\n",
    "#         {'Name': 'EvalRecall', 'Regex': r'\\'eval_recall\\':([-+]?[0-9]*\\.?[0-9]+(?:[eE][-+]?[0-9]+)?),'},\n",
    "\n",
    "#     ]\n",
    "\n",
    "##\n",
    "\n",
    "metric_definitions = [\n",
    "    {'Name': 'TrainLoss', 'Regex': '{\\'loss\\': (.*?),'},\n",
    "    {'Name': 'EvalLoss', 'Regex': '\\'eval_loss\\': (.*?),'},\n",
    "    {'Name': 'EvalAcc', 'Regex': '\\'eval_accuracy\\': (.*?),'},\n",
    "    {'Name': 'EvalF1', 'Regex': '\\'eval_f1\\': (.*?),'},\n",
    "    {'Name': 'EvalPrecision', 'Regex': '\\'eval_precision\\': (.*?),'},\n",
    "    {'Name': 'EvalRecall', 'Regex': '\\'eval_recall\\': (.*?),'},\n",
    "\n",
    "]\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                                    source_dir='./scripts',\n",
    "                                    instance_type=training_instance_type,\n",
    "                                    instance_count=training_instance_count,\n",
    "                                    role=role,\n",
    "                                    transformers_version='4.6',\n",
    "                                    pytorch_version='1.8',\n",
    "                                    py_version='py36',\n",
    "                                    hyperparameters = hyperparameters,\n",
    "                                    metric_definitions = metric_definitions,\n",
    "                                    image_uri=train_image_uri,\n",
    "                                    output_path=model_path,\n",
    "                                   )\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"HuggingFaceModelFineTune\",\n",
    "    estimator=huggingface_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    },\n",
    "    retry_policies=[\n",
    "        step_retry_policy,\n",
    "        job_retry_policy\n",
    "    ],\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_steps_list_train = [step_process, step_train]\n",
    "execution_train = sm_exec.exec_pipelines(pipeline_name, role, test_parameters_list, test_steps_list_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_exec.describe_pipelines(execution_train)\n",
    "sm_exec.get_step_results(execution_train,test_steps_list_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Register를 위한 단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inf_image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-cpu-py38-ubuntu20.04\"\n",
    "\n",
    "step_register = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    estimator=huggingface_estimator,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.c5.9xlarge\"], # instance types recommended by data scientist to be used for real-time endpoints\n",
    "    transform_instances=[\"ml.m5.12xlarge\", \"ml.c5.9xlarge\"], # instance types recommended by data scientist to be used for batch transform jobs\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    image_uri=inf_image_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_steps_list_register = [step_process, step_train, step_register]\n",
    "execution_register = sm_exec.exec_pipelines(pipeline_name, role, test_parameters_list, test_steps_list_register)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_exec.describe_pipelines(execution_register)\n",
    "sm_exec.get_step_results(execution_register,test_steps_list_register)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
